<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="The Lottery Ticket Hypothesis in Denoising: Towards Semantic-Driven Initialization">
  <meta name="keywords" content="Diffusion Model, Initial Noise, Layout-to-Image">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Guided Image Synthesis via Initial Image Editing in Diffusion Model</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

  
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Guided Image Synthesis via Initial Image Editing in Diffusion Model</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ut-mao.github.io/">Jiafeng Mao</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="hhttps://xueting-wang.github.io/">Xueting Wang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.hal.t.u-tokyo.ac.jp/lab/ja/index_1.xhtml">Kiyoharu Aizawa</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Tokyo,</span>
            <span class="author-block"><sup>2</sup>CyberAgent</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612191"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2305.03382"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              
              <span class="link-block">
                <a href="https://github.com/UT-Mao/Initial-Noise-Editing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/fig_0.png" alt="Image">
      <h2 class="subtitle has-text-centered">
        In this study, we investigate the impact of the initial image on image generation and propose a novel direction for controlling the generation process by manipulating the initial random noise. We present two direct applications of our findings: generated image re-painting and layout-to-image synthesis. Generated image re-painting allows users to modify a part of the generated image while preserving most of it. Layout-to-image synthesis requires generating objects in user-specified regions.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered">
      <div class="column is-full-widthy">
        <h2 class="title is-3"><span class="dnerf">Abstract</span></h2>
        <div class="content has-text-justified">
          
          <p>
            Diffusion models have the ability to generate high quality images by denoising pure Gaussian noise images. 
            While previous research has primarily focused on improving the control of image generation through adjusting 
            the denoising process, we propose a novel direction of manipulating the initial noise to control the generated image. 
            Through experiments on stable diffusion, we show that blocks of pixels in the initial latent images have a preference 
            for generating specific content, and that modifying these blocks can significantly influence the generated image. 
            In particular, we show that modifying a part of the initial image affects the corresponding region of the generated 
            image while leaving other regions unaffected, which is useful for repainting tasks. Furthermore, 
            we find that the generation preferences of pixel blocks are primarily determined by their values, 
            rather than their position. By moving pixel blocks with a tendency to generate user-desired content to 
            user-specified regions, our approach achieves state-of-the-art performance in layout-to-image generation. 
            Our results highlight the flexibility and power of initial image manipulation in controlling the generated image.
          </p>

        <h2 class="title is-3"><span class="dnerf">Generation Tendency of Initial Image</span></h2>  
        <img id="teaser" src="./static/images/method_1.png" alt="Image">
        <p>
          We first conduct experiments to check the generation tendency of initial images. 
          Specifically, we create a list of categories and then select two different categories, 
          <span class="dnerf">Cls1</span> and <span class="dnerf">Cls2</span>, from it to construct prompts for generation. 
          The prompt takes the form of "A <span class="dnerf">Cls1</span> and a <span class="dnerf">Cls2</span>" (e.g. a dog and a car). 
          We use fixed random initial images and a 
          model with fixed parameters to generate each constructed prompt and observe the generation performance of the 
          same initialization image guided by different prompts. When the same object is mentioned in different prompts, 
          there is a high probability that this object will be generated in the same location and have a very similar visual 
          appearance. 
        </p>

        <h2 class="title is-3"><span class="dnerf">Generated Image Re-painting</span></h2>  
        
        <p>
          A critical factor contributing to the failure of image generation is the 
          inconsistency between the initial noise image's generation tendency and the user-provided prompts. 
          For instance, if a user intends to generate two objects close to each other, 
          but the two regions that tend to generate these objects are far apart in the randomly generated initial image, 
          the model will fail to generate the desired content based on such an initial image.
        </p>
        <img id="teaser" src="./static/images/method_2.png" alt="Image">

        <p>
        Therefore, an intuitive idea to avoid image generation failure is to remove the part of the tendency that 
        conflicts with the prompt. First, we generate an image using prompt and a randomly sampled initialized image. 
        We then identify regions in the generated image that did not match the description or did not make sense. 
        We re-randomize the regions in the initialized image corresponding to the failure regions while keeping the 
        values of the other regions. We use the partially re-randomized initial noise image to perform 
        the generation again, under the same prompt, and observe the generated image. 
        This experiment is straightforward and intuitive yet shows a significant effect. Using the partially re-randomized 
        initial image achieves spontaneous regeneration of only the specified regions while keeping the other 
        regions of the image almost unchanged. 
        </p>
        <p>
        This result shows that the generation tendencies of each region in the initialized image are relatively independent, 
        and changing a small part of the initialized image does not affect the generation tendencies of the remaining part. 
        This experiment implies that by editing the initialized image, we can potentially influence the image generation 
        results in a directional and controlled manner.
        </p>
        <img id="teaser" src="./static/images/task_1.png" alt="Image">
          

        

        <h2 class="title is-3"><span class="dnerf">Layout-to-Image Synthesis</span></h2>
        <p>
          We use the attention map of the initial noise image to indicate the initial generation tendency. 
          Subsequently, we move the pixel blocks that tend to generate specific content into specified regions, 
          and the modified noise image is used to perform denoising as usual.
        </p>
        <img id="teaser" src="./static/images/method_3.png" alt="Image">
        <p>
          Our method achieves a comparable performance with state-of-the-art methods when used alone, 
          and combining our method with them achieves the best performance on all subsets.
        </p>

        


        <h2 class="title is-3"><span class="dnerf">Visual Samples</span></h2>

        <img id="teaser" src="./static/images/vis.png" alt="Image">

        <h2 class="title is-3"><span class="dnerf">Related Links</span></h2>
          <div class="content has-text-justified">
            <p>
              Refer to our latest work for more discussion about initial noise in diffusion!
            </p>
            <p>
              <a href="https://ut-mao.github.io/noise.github.io/">The Lottery Ticket Hypothesis in Denoising: Towards Semantic-Driven Initialization [ECCV 2024]</a> 
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{mao2023Guided,
  author    = {Jiafeng Mao, Xueting Wang and Kiyoharu Aizawa},
  title     = {Guided Image Synthesis via Initial Image Editing in Diffusion Model},
  journal   = {ACM MM},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
        <div class="content">
          <p>
            We thank the authors of <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> that kindly open sourced the template of this website.
          </p>
        </div>
      </div>
  </div>
</footer>



</body>
</html>
